{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperspectral Face Recognition Model\n",
    "\n",
    "This notebook implements a CNN-based hyperspectral face recognition model that extracts facial embeddings for authentication purposes.\n",
    "\n",
    "## Dataset Structure\n",
    "This implementation is designed for the UWA HSFD dataset structure:\n",
    "```\n",
    "dataset/\n",
    "├── subject_001/\n",
    "│   ├── image_001.npy (or .mat)\n",
    "│   └── image_002.npy\n",
    "└── subject_002/\n",
    "    └── ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_hyperspectral_data(data_path, img_size=(128, 128), num_channels=33):\n",
    "    \"\"\"\n",
    "    Load hyperspectral face data from the dataset directory.\n",
    "    \n",
    "    Args:\n",
    "        data_path: Path to the dataset directory\n",
    "        img_size: Target image size (height, width)\n",
    "        num_channels: Number of spectral channels (default 33 for UWA HSFD)\n",
    "    \n",
    "    Returns:\n",
    "        X: Array of hyperspectral images\n",
    "        y: Array of labels\n",
    "        subject_names: List of subject identifiers\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "    subject_names = []\n",
    "    \n",
    "    # For demonstration, create synthetic data if path doesn't exist\n",
    "    if not os.path.exists(data_path):\n",
    "        print(f\"Warning: Path {data_path} not found. Creating synthetic data for demonstration.\")\n",
    "        # Create synthetic data: 10 subjects with 5 images each\n",
    "        num_subjects = 10\n",
    "        images_per_subject = 5\n",
    "        \n",
    "        for subject_id in range(num_subjects):\n",
    "            subject_name = f\"subject_{subject_id:03d}\"\n",
    "            subject_names.append(subject_name)\n",
    "            \n",
    "            for img_id in range(images_per_subject):\n",
    "                # Generate synthetic hyperspectral data\n",
    "                img = np.random.randn(*img_size, num_channels).astype(np.float32)\n",
    "                # Add some structure to make it more realistic\n",
    "                img += np.random.randn(num_channels) * 0.5\n",
    "                X.append(img)\n",
    "                y.append(subject_id)\n",
    "        \n",
    "        return np.array(X), np.array(y), subject_names\n",
    "    \n",
    "    # Load real data\n",
    "    subject_dirs = sorted([d for d in os.listdir(data_path) \n",
    "                          if os.path.isdir(os.path.join(data_path, d))])\n",
    "    \n",
    "    for subject_id, subject_dir in enumerate(subject_dirs):\n",
    "        subject_names.append(subject_dir)\n",
    "        subject_path = os.path.join(data_path, subject_dir)\n",
    "        \n",
    "        image_files = [f for f in os.listdir(subject_path) \n",
    "                      if f.endswith('.npy') or f.endswith('.mat')]\n",
    "        \n",
    "        for img_file in image_files:\n",
    "            img_path = os.path.join(subject_path, img_file)\n",
    "            \n",
    "            if img_file.endswith('.npy'):\n",
    "                img = np.load(img_path)\n",
    "            elif img_file.endswith('.mat'):\n",
    "                from scipy.io import loadmat\n",
    "                data = loadmat(img_path)\n",
    "                # Adjust key based on your .mat file structure\n",
    "                img = data['hyperspectral_image']\n",
    "            \n",
    "            # Resize if needed\n",
    "            if img.shape[:2] != img_size:\n",
    "                from scipy.ndimage import zoom\n",
    "                zoom_factors = (img_size[0]/img.shape[0], \n",
    "                              img_size[1]/img.shape[1], 1)\n",
    "                img = zoom(img, zoom_factors, order=1)\n",
    "            \n",
    "            # Normalize\n",
    "            img = (img - img.mean()) / (img.std() + 1e-7)\n",
    "            \n",
    "            X.append(img)\n",
    "            y.append(subject_id)\n",
    "    \n",
    "    return np.array(X), np.array(y), subject_names\n",
    "\n",
    "# Load data (using synthetic data for demonstration)\n",
    "DATA_PATH = \"./UWA_HSFD_dataset\"  # Update this path for real data\n",
    "X, y, subject_names = load_hyperspectral_data(DATA_PATH)\n",
    "\n",
    "print(f\"Loaded {len(X)} images from {len(subject_names)} subjects\")\n",
    "print(f\"Image shape: {X[0].shape}\")\n",
    "print(f\"Number of classes: {len(np.unique(y))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build the Hyperspectral Face Recognition Model\n",
    "\n",
    "We'll create a CNN-based model with an embedding layer for feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_hyperspectral_model(input_shape=(128, 128, 33), embedding_dim=128, num_classes=10):\n",
    "    \"\"\"\n",
    "    Build a CNN model for hyperspectral face recognition.\n",
    "    \n",
    "    Args:\n",
    "        input_shape: Shape of input hyperspectral images\n",
    "        embedding_dim: Dimension of the embedding vector\n",
    "        num_classes: Number of subjects/classes for training\n",
    "    \n",
    "    Returns:\n",
    "        model: Complete model for training\n",
    "        embedding_model: Model that outputs embeddings only\n",
    "    \"\"\"\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    # Convolutional blocks for feature extraction\n",
    "    x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    x = layers.Dropout(0.25)(x)\n",
    "    \n",
    "    x = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    x = layers.Dropout(0.25)(x)\n",
    "    \n",
    "    x = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    x = layers.Dropout(0.25)(x)\n",
    "    \n",
    "    x = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    x = layers.Dropout(0.25)(x)\n",
    "    \n",
    "    # Flatten and create embedding\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(512, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    \n",
    "    # Embedding layer (L2 normalized for better similarity computation)\n",
    "    embeddings = layers.Dense(embedding_dim, activation=None, name='embeddings')(x)\n",
    "    embeddings_norm = layers.Lambda(lambda x: tf.nn.l2_normalize(x, axis=1), \n",
    "                                    name='embeddings_norm')(embeddings)\n",
    "    \n",
    "    # Classification head for training\n",
    "    outputs = layers.Dense(num_classes, activation='softmax', name='classification')(embeddings_norm)\n",
    "    \n",
    "    # Full model for training\n",
    "    model = models.Model(inputs=inputs, outputs=outputs, name='hyperspectral_face_model')\n",
    "    \n",
    "    # Embedding model for inference\n",
    "    embedding_model = models.Model(inputs=inputs, outputs=embeddings_norm, \n",
    "                                   name='embedding_model')\n",
    "    \n",
    "    return model, embedding_model\n",
    "\n",
    "# Build model\n",
    "IMG_SIZE = (128, 128)\n",
    "NUM_CHANNELS = 33\n",
    "EMBEDDING_DIM = 128\n",
    "NUM_CLASSES = len(np.unique(y))\n",
    "\n",
    "model, embedding_model = build_hyperspectral_model(\n",
    "    input_shape=(*IMG_SIZE, NUM_CHANNELS),\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    num_classes=NUM_CLASSES\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, \n",
    "                                                      stratify=y, random_state=42)\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")\n",
    "\n",
    "# Compile model\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Callbacks\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True),\n",
    "    keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5, min_lr=1e-7)\n",
    "]\n",
    "\n",
    "# Train model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=50,\n",
    "    batch_size=16,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].plot(history.history['loss'], label='Train Loss')\n",
    "axes[0].plot(history.history['val_loss'], label='Val Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training and Validation Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "axes[1].plot(history.history['accuracy'], label='Train Acc')\n",
    "axes[1].plot(history.history['val_accuracy'], label='Val Acc')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Training and Validation Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_history.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Evaluate on test set\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"\\nTest Accuracy: {test_acc:.4f}\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save the Model\n",
    "\n",
    "We save both the full model and the embedding model for use in the authentication system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save models\n",
    "model.save('hyperspectral_face_model.h5')\n",
    "embedding_model.save('hyperspectral_embedding_model.h5')\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    'img_size': IMG_SIZE,\n",
    "    'num_channels': NUM_CHANNELS,\n",
    "    'embedding_dim': EMBEDDING_DIM,\n",
    "    'num_classes': NUM_CLASSES,\n",
    "    'subject_names': subject_names,\n",
    "    'test_accuracy': test_acc\n",
    "}\n",
    "\n",
    "with open('model_metadata.pkl', 'wb') as f:\n",
    "    pickle.dump(metadata, f)\n",
    "\n",
    "print(\"Models saved successfully!\")\n",
    "print(f\"  - hyperspectral_face_model.h5\")\n",
    "print(f\"  - hyperspectral_embedding_model.h5\")\n",
    "print(f\"  - model_metadata.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test Embedding Extraction\n",
    "\n",
    "Verify that the embedding model works correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test embedding extraction\n",
    "sample_embeddings = embedding_model.predict(X_test[:5])\n",
    "\n",
    "print(f\"Embedding shape: {sample_embeddings.shape}\")\n",
    "print(f\"Sample embedding (first 10 values): {sample_embeddings[0][:10]}\")\n",
    "\n",
    "# Verify embeddings are L2 normalized\n",
    "norms = np.linalg.norm(sample_embeddings, axis=1)\n",
    "print(f\"\\nEmbedding norms (should be ~1.0): {norms}\")\n",
    "\n",
    "# Compute pairwise similarities\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "similarities = cosine_similarity(sample_embeddings)\n",
    "print(f\"\\nPairwise cosine similarities:\")\n",
    "print(similarities)\n",
    "\n",
    "print(\"\\n✓ Embedding model is ready for authentication system!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
