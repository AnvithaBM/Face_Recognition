{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Face Recognition Model for UWA HSFD Database\n",
    "\n",
    "This notebook implements a comprehensive face recognition system using deep learning techniques.\n",
    "The model is designed for the UWA Hyperspectral Face Database (UWA HSFD).\n",
    "\n",
    "## Table of Contents:\n",
    "1. Environment Setup and Dependencies\n",
    "2. Data Loading and Preprocessing\n",
    "3. Exploratory Data Analysis\n",
    "4. Data Augmentation\n",
    "5. Model Architecture\n",
    "6. Training Pipeline\n",
    "7. Model Evaluation\n",
    "8. Face Recognition and Authentication\n",
    "9. Results Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Deep Learning libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import ResNet50, VGG16\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Scikit-learn utilities\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure data path\n",
    "# Update this path to your local UWA HSFD database location\n",
    "data_path = r\"C:\\Users\\Anvitha\\Face based Person Authentication\\UWA HSFD V1.1 (1)\\UWA HSFD V1.1\\HyperSpec_Face_Session1\"\n",
    "\n",
    "# Image parameters\n",
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_hyperspectral_faces(data_path, img_size=(224, 224)):\n",
    "    \"\"\"\n",
    "    Load hyperspectral face images from the UWA HSFD database.\n",
    "    \n",
    "    Args:\n",
    "        data_path: Path to the hyperspectral face database\n",
    "        img_size: Target image size for resizing\n",
    "    \n",
    "    Returns:\n",
    "        images: Numpy array of processed images\n",
    "        labels: Numpy array of corresponding labels\n",
    "        label_names: Dictionary mapping label indices to person names\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    labels = []\n",
    "    label_names = {}\n",
    "    \n",
    "    if not os.path.exists(data_path):\n",
    "        print(f\"Warning: Data path '{data_path}' does not exist.\")\n",
    "        print(\"Creating sample synthetic data for demonstration...\")\n",
    "        return create_synthetic_data(img_size)\n",
    "    \n",
    "    # Walk through directory structure\n",
    "    for person_idx, person_name in enumerate(sorted(os.listdir(data_path))):\n",
    "        person_path = os.path.join(data_path, person_name)\n",
    "        \n",
    "        if os.path.isdir(person_path):\n",
    "            label_names[person_idx] = person_name\n",
    "            \n",
    "            # Load all images for this person\n",
    "            for img_file in os.listdir(person_path):\n",
    "                if img_file.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tif', '.tiff')):\n",
    "                    img_path = os.path.join(person_path, img_file)\n",
    "                    \n",
    "                    try:\n",
    "                        # Load and preprocess image\n",
    "                        img = cv2.imread(img_path)\n",
    "                        \n",
    "                        if img is not None:\n",
    "                            # Convert BGR to RGB\n",
    "                            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                            \n",
    "                            # Resize image\n",
    "                            img = cv2.resize(img, img_size)\n",
    "                            \n",
    "                            # Normalize pixel values to [0, 1]\n",
    "                            img = img.astype('float32') / 255.0\n",
    "                            \n",
    "                            images.append(img)\n",
    "                            labels.append(person_idx)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error loading {img_path}: {e}\")\n",
    "    \n",
    "    if len(images) == 0:\n",
    "        print(\"No images found. Creating sample synthetic data...\")\n",
    "        return create_synthetic_data(img_size)\n",
    "    \n",
    "    return np.array(images), np.array(labels), label_names\n",
    "\n",
    "def create_synthetic_data(img_size, num_persons=10, images_per_person=20):\n",
    "    \"\"\"\n",
    "    Create synthetic face data for demonstration purposes.\n",
    "    \"\"\"\n",
    "    print(f\"Creating synthetic dataset: {num_persons} persons, {images_per_person} images each\")\n",
    "    \n",
    "    images = []\n",
    "    labels = []\n",
    "    label_names = {i: f\"Person_{i+1}\" for i in range(num_persons)}\n",
    "    \n",
    "    for person_idx in range(num_persons):\n",
    "        for img_idx in range(images_per_person):\n",
    "            # Create synthetic face-like images with person-specific patterns\n",
    "            img = np.random.rand(img_size[0], img_size[1], 3).astype('float32')\n",
    "            \n",
    "            # Add person-specific pattern\n",
    "            pattern = np.sin(np.linspace(0, 2*np.pi*person_idx, img_size[0]))\n",
    "            img[:, :, 0] = img[:, :, 0] * 0.5 + pattern[:, np.newaxis] * 0.5\n",
    "            \n",
    "            images.append(img)\n",
    "            labels.append(person_idx)\n",
    "    \n",
    "    return np.array(images), np.array(labels), label_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "print(\"Loading hyperspectral face data...\")\n",
    "X, y, label_names = load_hyperspectral_faces(data_path, IMG_SIZE)\n",
    "\n",
    "print(f\"\\nDataset loaded successfully!\")\n",
    "print(f\"Number of images: {len(X)}\")\n",
    "print(f\"Image shape: {X[0].shape}\")\n",
    "print(f\"Number of unique persons: {len(np.unique(y))}\")\n",
    "print(f\"Label names: {label_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample images from each class\n",
    "def visualize_samples(X, y, label_names, samples_per_class=5):\n",
    "    \"\"\"\n",
    "    Visualize sample images from each person in the dataset.\n",
    "    \"\"\"\n",
    "    unique_labels = np.unique(y)\n",
    "    n_classes = len(unique_labels)\n",
    "    \n",
    "    fig, axes = plt.subplots(n_classes, samples_per_class, figsize=(15, 3*n_classes))\n",
    "    \n",
    "    if n_classes == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for idx, label in enumerate(unique_labels):\n",
    "        # Get indices for this label\n",
    "        label_indices = np.where(y == label)[0]\n",
    "        \n",
    "        # Sample random images\n",
    "        sample_indices = np.random.choice(label_indices, \n",
    "                                         min(samples_per_class, len(label_indices)), \n",
    "                                         replace=False)\n",
    "        \n",
    "        for i, img_idx in enumerate(sample_indices):\n",
    "            ax = axes[idx, i] if n_classes > 1 else axes[i]\n",
    "            ax.imshow(X[img_idx])\n",
    "            ax.axis('off')\n",
    "            \n",
    "            if i == 0:\n",
    "                ax.set_title(f\"{label_names[label]}\", fontsize=10, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle('Sample Images from Dataset', fontsize=14, y=1.001)\n",
    "    plt.show()\n",
    "\n",
    "visualize_samples(X, y, label_names, samples_per_class=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class distribution analysis\n",
    "def plot_class_distribution(y, label_names):\n",
    "    \"\"\"\n",
    "    Plot the distribution of images across different persons.\n",
    "    \"\"\"\n",
    "    unique, counts = np.unique(y, return_counts=True)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    bars = plt.bar([label_names[i] for i in unique], counts, color='skyblue', edgecolor='navy')\n",
    "    plt.xlabel('Person ID', fontsize=12)\n",
    "    plt.ylabel('Number of Images', fontsize=12)\n",
    "    plt.title('Class Distribution in Dataset', fontsize=14, fontweight='bold')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{int(height)}',\n",
    "                ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nClass Distribution Statistics:\")\n",
    "    print(f\"Mean images per person: {np.mean(counts):.2f}\")\n",
    "    print(f\"Std images per person: {np.std(counts):.2f}\")\n",
    "    print(f\"Min images per person: {np.min(counts)}\")\n",
    "    print(f\"Max images per person: {np.max(counts)}\")\n",
    "\n",
    "plot_class_distribution(y, label_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Preparation and Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train, validation, and test sets\n",
    "# First split: 80% train+val, 20% test\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Second split: 75% train, 25% val (of the temp set)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(\"Data Split:\")\n",
    "print(f\"Training set: {len(X_train)} images ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "print(f\"Validation set: {len(X_val)} images ({len(X_val)/len(X)*100:.1f}%)\")\n",
    "print(f\"Test set: {len(X_test)} images ({len(X_test)/len(X)*100:.1f}%)\")\n",
    "\n",
    "# Convert labels to categorical\n",
    "num_classes = len(np.unique(y))\n",
    "y_train_cat = to_categorical(y_train, num_classes)\n",
    "y_val_cat = to_categorical(y_val, num_classes)\n",
    "y_test_cat = to_categorical(y_test, num_classes)\n",
    "\n",
    "print(f\"\\nNumber of classes: {num_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    zoom_range=0.15,\n",
    "    shear_range=0.15,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# No augmentation for validation and test sets\n",
    "val_datagen = ImageDataGenerator()\n",
    "test_datagen = ImageDataGenerator()\n",
    "\n",
    "print(\"Data augmentation configured successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_face_recognition_model(input_shape=(224, 224, 3), num_classes=10):\n",
    "    \"\"\"\n",
    "    Create a deep learning model for face recognition.\n",
    "    Uses transfer learning with ResNet50 as the base.\n",
    "    \n",
    "    Args:\n",
    "        input_shape: Shape of input images\n",
    "        num_classes: Number of persons to recognize\n",
    "    \n",
    "    Returns:\n",
    "        model: Compiled Keras model\n",
    "    \"\"\"\n",
    "    # Load pre-trained ResNet50 (without top layers)\n",
    "    base_model = ResNet50(\n",
    "        weights='imagenet',\n",
    "        include_top=False,\n",
    "        input_shape=input_shape\n",
    "    )\n",
    "    \n",
    "    # Freeze base model layers initially\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    # Build custom top layers\n",
    "    model = models.Sequential([\n",
    "        base_model,\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def create_custom_cnn_model(input_shape=(224, 224, 3), num_classes=10):\n",
    "    \"\"\"\n",
    "    Create a custom CNN model from scratch for face recognition.\n",
    "    \n",
    "    Args:\n",
    "        input_shape: Shape of input images\n",
    "        num_classes: Number of persons to recognize\n",
    "    \n",
    "    Returns:\n",
    "        model: Compiled Keras model\n",
    "    \"\"\"\n",
    "    model = models.Sequential([\n",
    "        # First Conv Block\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape, padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        # Second Conv Block\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        # Third Conv Block\n",
    "        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        # Fourth Conv Block\n",
    "        layers.Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        # Fully Connected Layers\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model - Choose one: Transfer Learning (faster, better) or Custom CNN\n",
    "# Option 1: Transfer Learning with ResNet50 (Recommended)\n",
    "print(\"Creating face recognition model with Transfer Learning (ResNet50)...\")\n",
    "model = create_face_recognition_model(input_shape=(224, 224, 3), num_classes=num_classes)\n",
    "\n",
    "# Option 2: Custom CNN (uncomment to use)\n",
    "# print(\"Creating custom CNN model...\")\n",
    "# model = create_custom_cnn_model(input_shape=(224, 224, 3), num_classes=num_classes)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Display model architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callbacks\n",
    "callbacks = [\n",
    "    # Save best model\n",
    "    ModelCheckpoint(\n",
    "        'best_face_recognition_model.h5',\n",
    "        monitor='val_accuracy',\n",
    "        mode='max',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    # Early stopping to prevent overfitting\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    # Reduce learning rate when plateau\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"Training callbacks configured successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "print(\"Starting model training...\")\n",
    "print(f\"Training for {EPOCHS} epochs with batch size {BATCH_SIZE}\")\n",
    "\n",
    "history = model.fit(\n",
    "    train_datagen.flow(X_train, y_train_cat, batch_size=BATCH_SIZE),\n",
    "    validation_data=(X_val, y_val_cat),\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nTraining completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning: Unfreeze some layers and retrain with lower learning rate\n",
    "def fine_tune_model(model, X_train, y_train_cat, X_val, y_val_cat, epochs=20):\n",
    "    \"\"\"\n",
    "    Fine-tune the model by unfreezing some base layers.\n",
    "    \"\"\"\n",
    "    # Unfreeze the base model\n",
    "    base_model = model.layers[0]\n",
    "    if hasattr(base_model, 'trainable'):\n",
    "        base_model.trainable = True\n",
    "        \n",
    "        # Freeze early layers, unfreeze later layers\n",
    "        for layer in base_model.layers[:-30]:\n",
    "            layer.trainable = False\n",
    "    \n",
    "    # Recompile with lower learning rate\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.0001),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    print(\"Fine-tuning model...\")\n",
    "    \n",
    "    history_fine = model.fit(\n",
    "        train_datagen.flow(X_train, y_train_cat, batch_size=BATCH_SIZE),\n",
    "        validation_data=(X_val, y_val_cat),\n",
    "        epochs=epochs,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    return history_fine\n",
    "\n",
    "# Uncomment to perform fine-tuning\n",
    "# history_fine = fine_tune_model(model, X_train, y_train_cat, X_val, y_val_cat, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "def plot_training_history(history):\n",
    "    \"\"\"\n",
    "    Plot training and validation accuracy and loss.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Plot accuracy\n",
    "    axes[0].plot(history.history['accuracy'], label='Training Accuracy', linewidth=2)\n",
    "    axes[0].plot(history.history['val_accuracy'], label='Validation Accuracy', linewidth=2)\n",
    "    axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[0].set_ylabel('Accuracy', fontsize=12)\n",
    "    axes[0].set_title('Model Accuracy', fontsize=14, fontweight='bold')\n",
    "    axes[0].legend(fontsize=10)\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot loss\n",
    "    axes[1].plot(history.history['loss'], label='Training Loss', linewidth=2)\n",
    "    axes[1].plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "    axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[1].set_ylabel('Loss', fontsize=12)\n",
    "    axes[1].set_title('Model Loss', fontsize=14, fontweight='bold')\n",
    "    axes[1].legend(fontsize=10)\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "print(\"Evaluating model on test set...\")\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test_cat, verbose=1)\n",
    "\n",
    "print(f\"\\nTest Results:\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "y_pred_probs = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(\"=\" * 70)\n",
    "target_names = [label_names[i] for i in sorted(label_names.keys())]\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "\n",
    "# Calculate additional metrics\n",
    "precision, recall, f1, support = precision_recall_fscore_support(y_test, y_pred, average='weighted')\n",
    "print(f\"\\nWeighted Metrics:\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "def plot_confusion_matrix(y_true, y_pred, label_names):\n",
    "    \"\"\"\n",
    "    Plot confusion matrix for model predictions.\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=[label_names[i] for i in sorted(label_names.keys())],\n",
    "                yticklabels=[label_names[i] for i in sorted(label_names.keys())],\n",
    "                cbar_kws={'label': 'Count'})\n",
    "    plt.xlabel('Predicted Label', fontsize=12)\n",
    "    plt.ylabel('True Label', fontsize=12)\n",
    "    plt.title('Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate per-class accuracy\n",
    "    class_accuracy = cm.diagonal() / cm.sum(axis=1)\n",
    "    print(\"\\nPer-Class Accuracy:\")\n",
    "    for i, acc in enumerate(class_accuracy):\n",
    "        print(f\"{label_names[i]}: {acc*100:.2f}%\")\n",
    "\n",
    "plot_confusion_matrix(y_test, y_pred, label_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Face Recognition and Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceAuthenticator:\n",
    "    \"\"\"\n",
    "    Face authentication system for verifying person identity.\n",
    "    \"\"\"\n",
    "    def __init__(self, model, label_names, threshold=0.7):\n",
    "        \"\"\"\n",
    "        Initialize the authenticator.\n",
    "        \n",
    "        Args:\n",
    "            model: Trained face recognition model\n",
    "            label_names: Dictionary mapping label indices to person names\n",
    "            threshold: Confidence threshold for authentication (0-1)\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.label_names = label_names\n",
    "        self.threshold = threshold\n",
    "    \n",
    "    def preprocess_image(self, img_path, img_size=(224, 224)):\n",
    "        \"\"\"\n",
    "        Preprocess an image for authentication.\n",
    "        \"\"\"\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.resize(img, img_size)\n",
    "        img = img.astype('float32') / 255.0\n",
    "        img = np.expand_dims(img, axis=0)\n",
    "        return img\n",
    "    \n",
    "    def authenticate(self, img_input, claimed_identity=None):\n",
    "        \"\"\"\n",
    "        Authenticate a person from an image.\n",
    "        \n",
    "        Args:\n",
    "            img_input: Image array or path to image\n",
    "            claimed_identity: Expected person ID (for verification mode)\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with authentication results\n",
    "        \"\"\"\n",
    "        # Preprocess if path provided\n",
    "        if isinstance(img_input, str):\n",
    "            img = self.preprocess_image(img_input)\n",
    "        else:\n",
    "            img = np.expand_dims(img_input, axis=0) if len(img_input.shape) == 3 else img_input\n",
    "        \n",
    "        # Get predictions\n",
    "        predictions = self.model.predict(img, verbose=0)\n",
    "        predicted_class = np.argmax(predictions[0])\n",
    "        confidence = predictions[0][predicted_class]\n",
    "        \n",
    "        # Authentication result\n",
    "        result = {\n",
    "            'predicted_identity': self.label_names[predicted_class],\n",
    "            'predicted_id': int(predicted_class),\n",
    "            'confidence': float(confidence),\n",
    "            'authenticated': confidence >= self.threshold,\n",
    "            'all_probabilities': {self.label_names[i]: float(predictions[0][i]) \n",
    "                                 for i in range(len(predictions[0]))}\n",
    "        }\n",
    "        \n",
    "        # Verification mode\n",
    "        if claimed_identity is not None:\n",
    "            result['claimed_identity'] = claimed_identity\n",
    "            result['identity_match'] = (predicted_class == claimed_identity)\n",
    "            result['verified'] = (result['identity_match'] and result['authenticated'])\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def batch_authenticate(self, images):\n",
    "        \"\"\"\n",
    "        Authenticate multiple images.\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        for img in images:\n",
    "            results.append(self.authenticate(img))\n",
    "        return results\n",
    "\n",
    "# Create authenticator instance\n",
    "authenticator = FaceAuthenticator(model, label_names, threshold=0.7)\n",
    "print(\"Face Authenticator created successfully!\")\n",
    "print(f\"Authentication threshold: {authenticator.threshold}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test authentication on random test samples\n",
    "def test_authentication(authenticator, X_test, y_test, label_names, num_samples=10):\n",
    "    \"\"\"\n",
    "    Test authentication on random samples from test set.\n",
    "    \"\"\"\n",
    "    sample_indices = np.random.choice(len(X_test), num_samples, replace=False)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 5, figsize=(20, 8))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for idx, sample_idx in enumerate(sample_indices):\n",
    "        img = X_test[sample_idx]\n",
    "        true_label = y_test[sample_idx]\n",
    "        \n",
    "        # Authenticate\n",
    "        result = authenticator.authenticate(img, claimed_identity=true_label)\n",
    "        \n",
    "        # Display\n",
    "        axes[idx].imshow(img)\n",
    "        axes[idx].axis('off')\n",
    "        \n",
    "        title_color = 'green' if result['verified'] else 'red'\n",
    "        title = f\"True: {label_names[true_label]}\\n\"\n",
    "        title += f\"Pred: {result['predicted_identity']}\\n\"\n",
    "        title += f\"Conf: {result['confidence']:.2%}\\n\"\n",
    "        title += f\"Status: {'✓ VERIFIED' if result['verified'] else '✗ REJECTED'}\"\n",
    "        \n",
    "        axes[idx].set_title(title, fontsize=9, color=title_color, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle('Face Authentication Test Results', fontsize=14, fontweight='bold', y=1.001)\n",
    "    plt.show()\n",
    "\n",
    "test_authentication(authenticator, X_test, y_test, label_names, num_samples=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate authentication metrics\n",
    "def evaluate_authentication_system(authenticator, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Evaluate authentication system performance.\n",
    "    \"\"\"\n",
    "    results = authenticator.batch_authenticate(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    total = len(results)\n",
    "    authenticated = sum(1 for r in results if r['authenticated'])\n",
    "    correct_predictions = sum(1 for i, r in enumerate(results) if r['predicted_id'] == y_test[i])\n",
    "    \n",
    "    # For verification scenario\n",
    "    verified_results = [authenticator.authenticate(X_test[i], claimed_identity=y_test[i]) \n",
    "                       for i in range(len(X_test))]\n",
    "    verified = sum(1 for r in verified_results if r['verified'])\n",
    "    \n",
    "    print(\"Authentication System Performance:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Total test samples: {total}\")\n",
    "    print(f\"Authenticated (confidence ≥ threshold): {authenticated} ({authenticated/total*100:.2f}%)\")\n",
    "    print(f\"Correct predictions: {correct_predictions} ({correct_predictions/total*100:.2f}%)\")\n",
    "    print(f\"Verified (correct + confident): {verified} ({verified/total*100:.2f}%)\")\n",
    "    print(f\"\\nFalse Rejection Rate (FRR): {(total-verified)/total*100:.2f}%\")\n",
    "    \n",
    "    # Confidence distribution\n",
    "    confidences = [r['confidence'] for r in results]\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(confidences, bins=20, color='skyblue', edgecolor='navy')\n",
    "    plt.axvline(authenticator.threshold, color='red', linestyle='--', linewidth=2, label='Threshold')\n",
    "    plt.xlabel('Confidence Score', fontsize=12)\n",
    "    plt.ylabel('Frequency', fontsize=12)\n",
    "    plt.title('Confidence Score Distribution', fontsize=14, fontweight='bold')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    metrics = ['Authenticated', 'Correct\\nPredictions', 'Verified']\n",
    "    values = [authenticated/total*100, correct_predictions/total*100, verified/total*100]\n",
    "    bars = plt.bar(metrics, values, color=['skyblue', 'lightgreen', 'coral'], edgecolor='navy')\n",
    "    plt.ylabel('Percentage (%)', fontsize=12)\n",
    "    plt.title('Authentication Metrics', fontsize=14, fontweight='bold')\n",
    "    plt.ylim(0, 105)\n",
    "    \n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.1f}%',\n",
    "                ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "evaluate_authentication_system(authenticator, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model Persistence and Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the complete model\n",
    "model.save('face_recognition_model_complete.h5')\n",
    "print(\"Complete model saved as 'face_recognition_model_complete.h5'\")\n",
    "\n",
    "# Save model in TensorFlow SavedModel format (recommended for deployment)\n",
    "model.save('face_recognition_model_savedmodel', save_format='tf')\n",
    "print(\"Model saved in TensorFlow SavedModel format\")\n",
    "\n",
    "# Save label names\n",
    "import json\n",
    "with open('label_names.json', 'w') as f:\n",
    "    json.dump(label_names, f, indent=2)\n",
    "print(\"Label names saved as 'label_names.json'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model for inference (example)\n",
    "def load_model_for_inference(model_path='face_recognition_model_complete.h5', \n",
    "                            label_path='label_names.json'):\n",
    "    \"\"\"\n",
    "    Load a saved model for inference.\n",
    "    \"\"\"\n",
    "    loaded_model = keras.models.load_model(model_path)\n",
    "    \n",
    "    with open(label_path, 'r') as f:\n",
    "        loaded_labels = json.load(f)\n",
    "        # Convert string keys back to integers\n",
    "        loaded_labels = {int(k): v for k, v in loaded_labels.items()}\n",
    "    \n",
    "    return loaded_model, loaded_labels\n",
    "\n",
    "# Example: Load and test\n",
    "# loaded_model, loaded_labels = load_model_for_inference()\n",
    "# print(\"Model loaded successfully for inference!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary and Next Steps\n",
    "\n",
    "### Model Performance Summary:\n",
    "This notebook has successfully implemented a deep learning-based face recognition system with the following components:\n",
    "\n",
    "1. **Data Pipeline**: Robust data loading and preprocessing for hyperspectral face images\n",
    "2. **Model Architecture**: Transfer learning with ResNet50 or custom CNN\n",
    "3. **Training**: Complete training pipeline with data augmentation and callbacks\n",
    "4. **Evaluation**: Comprehensive metrics including accuracy, precision, recall, and F1-score\n",
    "5. **Authentication**: Face authentication system with confidence thresholding\n",
    "\n",
    "### Key Features:\n",
    "- Data augmentation for improved generalization\n",
    "- Transfer learning for faster convergence\n",
    "- Authentication with confidence thresholding\n",
    "- Comprehensive evaluation metrics\n",
    "- Model persistence for deployment\n",
    "\n",
    "### Next Steps for Production:\n",
    "1. **Collect More Data**: Increase dataset size for better generalization\n",
    "2. **Face Detection**: Add face detection preprocessing (e.g., MTCNN, Haar Cascades)\n",
    "3. **Face Alignment**: Implement face alignment for better recognition\n",
    "4. **Liveness Detection**: Add anti-spoofing measures\n",
    "5. **API Development**: Create REST API for model deployment\n",
    "6. **Real-time Processing**: Optimize for real-time video stream processing\n",
    "7. **Database Integration**: Connect to authentication database\n",
    "8. **Logging and Monitoring**: Add comprehensive logging system\n",
    "\n",
    "### Usage Instructions:\n",
    "```python\n",
    "# To use the saved model:\n",
    "model, labels = load_model_for_inference()\n",
    "authenticator = FaceAuthenticator(model, labels, threshold=0.7)\n",
    "\n",
    "# Authenticate a face:\n",
    "result = authenticator.authenticate('path/to/face/image.jpg')\n",
    "print(f\"Identity: {result['predicted_identity']}\")\n",
    "print(f\"Confidence: {result['confidence']:.2%}\")\n",
    "print(f\"Authenticated: {result['authenticated']}\")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
